{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  # Load the training, validation and test data into constants that are\n",
    "  # attached to the graph.\n",
    "  tf_train_dataset = tf.constant(train_dataset[:train_subset, :])\n",
    "  tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  # These are the parameters that we are going to be training. The weight\n",
    "  # matrix will be initialized using random values following a (truncated)\n",
    "  # normal distribution. The biases get initialized to zero.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "  # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "  # it's very common, and it can be optimized). We take the average of this\n",
    "  # cross-entropy across all training examples: that's our loss.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  # We are going to find the minimum of this loss using gradient descent.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  # These are not part of training, but merely here so that we can report\n",
    "  # accuracy figures as we train.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Loss at step 0: 17.663475\n",
      "Training accuracy: 11.1%\n",
      "Validation accuracy: 14.9%\n",
      "Loss at step 100: 2.381060\n",
      "Training accuracy: 71.3%\n",
      "Validation accuracy: 70.7%\n",
      "Loss at step 200: 1.914271\n",
      "Training accuracy: 74.0%\n",
      "Validation accuracy: 73.2%\n",
      "Loss at step 300: 1.666182\n",
      "Training accuracy: 75.6%\n",
      "Validation accuracy: 74.3%\n",
      "Loss at step 400: 1.497755\n",
      "Training accuracy: 76.7%\n",
      "Validation accuracy: 74.6%\n",
      "Loss at step 500: 1.371619\n",
      "Training accuracy: 77.2%\n",
      "Validation accuracy: 74.8%\n",
      "Loss at step 600: 1.271889\n",
      "Training accuracy: 77.7%\n",
      "Validation accuracy: 75.1%\n",
      "Loss at step 700: 1.190417\n",
      "Training accuracy: 78.3%\n",
      "Validation accuracy: 75.3%\n",
      "Loss at step 800: 1.122355\n",
      "Training accuracy: 78.7%\n",
      "Validation accuracy: 75.4%\n",
      "Test accuracy: 82.1%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # This is a one-time operation which ensures the parameters get initialized as\n",
    "  # we described in the graph: random weights for the matrix, zeros for the\n",
    "  # biases. \n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "    # and get the loss value and the training predictions returned as numpy\n",
    "    # arrays.\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "    if (step % 100 == 0):\n",
    "      print('Loss at step %d: %f' % (step, l))\n",
    "      print('Training accuracy: %.1f%%' % accuracy(\n",
    "        predictions, train_labels[:train_subset, :]))\n",
    "      # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "      # just to get that one numpy array. Note that it recomputes all its graph\n",
    "      # dependencies.\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 17.501194\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 13.2%\n",
      "Minibatch loss at step 500: 1.944577\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 76.0%\n",
      "Minibatch loss at step 1000: 1.254470\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 1500: 0.676433\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 2000: 1.350471\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 2500: 0.991967\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 3000: 1.099635\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 79.1%\n",
      "Test accuracy: 85.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Softmax:0\", shape=(128, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_nodes = 1024\n",
    "graphRelu = tf.Graph()\n",
    "\n",
    "with graphRelu.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_nodes]))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_nodes]))\n",
    "  \n",
    "  weights_2 = tf.Variable( tf.truncated_normal([num_nodes, num_labels]))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  logits_1 = tf.matmul(tf_train_dataset, weights_1) + biases_1\n",
    "  relu_layer = tf.nn.relu(logits_1)  \n",
    "  logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "   \n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  print(train_prediction)\n",
    "  valid_prediction = tf.nn.softmax(logits_2)\n",
    "  # Predictions for test\n",
    "  logits_1 = tf.matmul(tf_test_dataset, weights_1) + biases_1\n",
    "  relu_layer= tf.nn.relu(logits_1)\n",
    "  logits_2 = tf.matmul(relu_layer, weights_2) + biases_2\n",
    "  test_prediction =  tf.nn.softmax(logits_2)\n",
    "  train_prediction = tf.nn.softmax(logits_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 10)\n",
      "(10000, 10)\n",
      "Initialized\n",
      "Minibatch loss at step 0: 358.250763\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/6cda136a-89bc-4288-9529-138a103aedae/ml/conda/envs/artem/lib/python3.5/site-packages/ipykernel_launcher.py:5: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 5: 305.397949\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 10: 135.690613\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 15: 62.625008\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 20: 117.838486\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 25: 102.443977\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 30: 70.167953\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 35: 38.942970\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 40: 27.450523\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 45: 47.594177\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 50: 25.047895\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 55: 99.163818\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 60: 46.372433\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 65: 77.271622\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 70: 61.529797\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 75: 34.022797\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 80: 40.780418\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 85: 33.659481\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 90: 23.948353\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 95: 24.495831\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 100: 37.457428\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 105: 48.797020\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 110: 34.306068\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 115: 36.921898\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 120: 38.503197\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 125: 31.684002\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 130: 29.466637\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 135: 33.934284\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 140: 25.963573\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 145: 106.934235\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 150: 39.706383\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 155: 20.702415\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 160: 40.218410\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 165: 28.533148\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 170: 26.788219\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 175: 31.040800\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 180: 21.105022\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 185: 16.790680\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 190: 31.377317\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 195: 28.388517\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 200: 38.189842\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 205: 31.249882\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 210: 38.635971\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 215: 26.720644\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 220: 87.255943\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 225: 16.256443\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 230: 15.827127\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 235: 66.901917\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 240: 43.187454\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 245: 26.699417\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 250: 14.070288\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 255: 19.922882\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 260: 19.763729\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 265: 35.470196\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 270: 35.612671\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 275: 28.588329\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 280: 14.972756\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 285: 26.437857\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 290: 16.931620\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 295: 31.547976\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 300: 12.084007\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 305: 17.472103\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 310: 13.807816\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 315: 21.685240\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 320: 31.171434\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 325: 15.248137\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 330: 19.312029\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 335: 17.086824\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 340: 48.148678\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 345: 42.848255\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 350: 35.069839\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 355: 48.262547\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 360: 13.258549\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 365: 21.287960\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 370: 20.203373\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 375: 17.989529\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 380: 26.197950\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 385: 12.503544\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 390: 41.585281\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 395: 8.805525\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 400: 17.139956\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 405: 13.451616\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 410: 35.351231\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 415: 13.438399\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 420: 22.799530\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 425: 7.835634\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 430: 49.911453\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 435: 34.045849\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 440: 24.136934\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 445: 14.792067\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 450: 12.054295\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 455: 65.421356\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 460: 20.099499\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 465: 10.442272\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 470: 11.819839\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 475: 10.065594\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 480: 15.592271\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 485: 20.659042\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 490: 20.318192\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 495: 18.935520\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 500: 19.788710\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 505: 14.231821\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 510: 9.583282\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 515: 19.335846\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 520: 15.821375\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 525: 20.225801\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 530: 11.149410\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 535: 15.170874\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 540: 10.889720\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 545: 12.879215\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 550: 10.604738\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 555: 17.713736\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 560: 12.970810\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 565: 15.213876\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 570: 25.550791\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 575: 23.116045\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 580: 31.560499\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 585: 6.137337\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 590: 8.015117\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 595: 19.366655\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 600: 6.360716\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 605: 21.132816\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 610: 9.409067\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 615: 12.539755\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 620: 9.528286\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 625: 16.585958\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 630: 10.760970\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 635: 22.075537\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 640: 6.945936\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 645: 9.948013\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 650: 11.012611\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 655: 17.792057\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 660: 12.415359\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 665: 12.255231\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 670: 8.681498\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 675: 9.836360\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 680: 46.832302\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 685: 9.407408\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 690: 8.482702\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 695: 8.449996\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 700: 22.559143\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 705: 16.364983\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 710: 9.138935\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 715: 15.997431\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 720: 12.141090\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 725: 16.933239\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 730: 7.749812\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 735: 9.317839\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 740: 50.944336\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 745: 10.624839\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 750: 10.725274\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 755: 10.616100\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 760: 13.149614\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 765: 14.926846\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 770: 9.788813\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 775: 12.501069\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 780: 7.456768\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 785: 5.563298\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 790: 17.093227\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 795: 8.478474\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 800: 10.596825\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 805: 20.887386\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 810: 9.359676\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 815: 4.859975\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 820: 16.347210\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 825: 3.440825\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 830: 7.199683\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 835: 4.950758\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 840: 8.169949\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 845: 9.864915\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 850: 24.595633\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 855: 5.258785\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 860: 13.417949\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 865: 10.329161\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 870: 26.319176\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 875: 7.272342\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 880: 21.017746\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 885: 6.576161\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 890: 9.308250\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 895: 14.196653\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 900: 9.067537\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 905: 5.675009\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 910: 5.851407\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 915: 5.485826\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 920: 9.525381\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 925: 5.787236\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 930: 12.737261\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 935: 7.794137\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 940: 13.532990\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 945: 19.895912\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 950: 10.548654\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 955: 3.563855\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 960: 15.462658\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 965: 5.637544\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 970: 5.428092\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 975: 13.863178\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 980: 5.532297\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 985: 10.571918\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 990: 6.444977\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 995: 21.652615\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1000: 8.671513\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1005: 7.701881\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1010: 7.651127\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1015: 5.188734\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1020: 13.834684\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1025: 11.304424\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1030: 13.790697\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1035: 5.332298\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1040: 9.291748\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1045: 6.795342\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1050: 18.474943\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1055: 5.673353\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1060: 5.806857\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1065: 6.823770\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1070: 16.879303\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1075: 6.800797\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1080: 18.260403\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1085: 4.010830\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1090: 5.126531\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1095: 16.504200\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1100: 12.427094\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1105: 24.710611\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1110: 6.579988\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1115: 8.825588\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1120: 13.226059\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1125: 12.749498\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1130: 18.802612\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1135: 22.890007\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1140: 10.739503\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1145: 5.136534\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1150: 3.420491\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1155: 18.864822\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1160: 11.101324\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1165: 6.469368\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1170: 8.503130\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1175: 4.574208\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1180: 7.581007\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1185: 21.154278\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1190: 7.379446\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1195: 6.464437\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1200: 6.362245\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1205: 7.052725\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1210: 7.928711\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1215: 7.712951\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1220: 6.922361\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1225: 11.026031\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1230: 5.425180\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1235: 8.755374\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1240: 6.030130\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1245: 7.685003\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1250: 6.956775\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1255: 7.928361\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1260: 14.030918\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1265: 7.711453\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1270: 9.618427\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1275: 8.899231\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1280: 7.856520\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1285: 12.389768\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1290: 4.627541\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1295: 4.380841\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1300: 5.187869\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1305: 4.421593\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1310: 6.219644\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1315: 4.968367\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1320: 11.174778\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1325: 4.740682\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1330: 10.892947\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1335: 16.287712\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1340: 5.575028\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1345: 5.064194\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1350: 8.038263\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1355: 7.299390\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1360: 9.295388\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1365: 5.657482\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1370: 6.484992\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1375: 9.342964\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1380: 7.863442\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1385: 10.098782\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1390: 4.455698\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1395: 10.594288\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1400: 25.772839\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1405: 9.470969\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1410: 8.027564\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1415: 5.502119\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1420: 6.122803\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1425: 9.298802\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1430: 7.593166\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1435: 28.147511\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1440: 5.298287\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1445: 6.398288\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1450: 4.466481\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1455: 6.727561\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1460: 6.200956\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1465: 6.003713\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1470: 2.575463\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1475: 3.771225\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1480: 9.506715\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1485: 4.686025\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1490: 7.244602\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1495: 8.659279\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1500: 6.608846\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1505: 23.523991\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1510: 7.180753\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1515: 12.330544\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1520: 7.744452\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1525: 10.980598\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1530: 13.336228\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1535: 4.564441\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1540: 14.234997\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1545: 8.527347\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1550: 11.023710\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1555: 2.982134\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1560: 11.054845\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1565: 3.746808\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1570: 9.283777\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1575: 4.801309\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1580: 5.209785\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1585: 4.532519\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1590: 5.500536\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1595: 11.638477\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1600: 10.109653\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1605: 7.698354\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1610: 5.023229\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1615: 3.452156\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1620: 7.294212\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1625: 9.704735\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1630: 6.471024\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1635: 5.815800\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1640: 5.656608\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1645: 11.276386\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1650: 6.250777\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1655: 8.746346\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1660: 5.580332\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1665: 8.009669\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1670: 4.485914\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1675: 6.923827\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1680: 2.402955\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1685: 3.505095\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1690: 4.284170\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1695: 5.050142\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1700: 6.931262\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1705: 3.997578\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1710: 6.727804\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1715: 7.397204\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1720: 16.307306\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1725: 2.513103\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1730: 6.346208\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1735: 5.169764\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1740: 7.592108\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1745: 3.720772\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1750: 5.460326\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1755: 3.715204\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1760: 7.634585\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1765: 3.663422\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1770: 3.374868\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1775: 2.493884\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1780: 3.827937\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1785: 3.754287\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1790: 1.198296\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1795: 9.935106\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1800: 5.321716\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1805: 1.756304\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1810: 5.276428\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1815: 7.548788\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1820: 14.951135\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1825: 4.396116\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1830: 4.975154\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1835: 9.737606\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1840: 10.483404\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1845: 14.829839\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1850: 14.200527\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1855: 3.274910\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1860: 5.501717\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1865: 6.367873\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1870: 3.464800\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1875: 2.236627\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1880: 10.015249\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1885: 7.126243\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1890: 6.298566\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1895: 3.374224\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1900: 9.831852\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1905: 10.681703\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1910: 5.211272\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1915: 11.964855\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1920: 3.027615\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1925: 4.015557\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1930: 2.419853\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1935: 2.913809\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1940: 9.291224\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1945: 5.693555\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1950: 2.152656\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1955: 3.307347\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1960: 3.865720\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 1965: 6.408886\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1970: 7.564520\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1975: 3.121014\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1980: 6.151566\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1985: 7.095388\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1990: 7.120298\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 1995: 3.079757\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2000: 3.956807\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2005: 4.200720\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2010: 4.223074\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2015: 2.688732\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2020: 4.822520\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2025: 4.804189\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2030: 15.628176\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2035: 4.804468\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2040: 2.567997\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2045: 3.243645\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2050: 1.581792\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2055: 5.467326\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2060: 7.378095\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2065: 6.523327\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2070: 7.025024\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2075: 2.169667\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2080: 3.431902\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2085: 7.984015\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2090: 10.100280\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2095: 6.265154\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2100: 4.200399\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2105: 10.535940\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2110: 1.743536\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2115: 5.252630\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2120: 5.318290\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2125: 8.065971\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2130: 6.553561\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2135: 4.891287\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2140: 5.479940\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2145: 9.440514\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2150: 6.763402\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2155: 3.446837\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2160: 10.611856\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2165: 5.545141\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2170: 1.643551\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2175: 4.859543\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2180: 3.681968\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2185: 1.352185\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2190: 2.441168\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2195: 5.013945\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2200: 4.071417\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2205: 12.781120\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2210: 6.416147\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2215: 5.628600\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2220: 3.618166\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2225: 4.567007\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2230: 5.779008\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2235: 5.148811\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2240: 5.037669\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2245: 3.227864\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2250: 2.527162\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2255: 3.982829\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2260: 2.725870\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2265: 3.460489\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2270: 2.980865\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2275: 2.014458\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2280: 9.180447\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2285: 5.779600\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2290: 12.292695\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2295: 5.280723\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2300: 5.595178\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2305: 2.951833\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2310: 4.291920\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2315: 6.676875\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2320: 4.984272\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2325: 5.271537\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2330: 16.838547\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2335: 3.037556\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2340: 13.016193\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2345: 5.607665\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2350: 5.128981\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2355: 5.575204\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2360: 9.539059\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2365: 4.622822\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2370: 6.202603\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2375: 7.583590\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2380: 9.975784\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2385: 4.977818\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2390: 3.610077\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2395: 4.673368\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2400: 3.690544\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2405: 5.263488\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2410: 4.754863\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2415: 3.305224\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2420: 6.791213\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2425: 5.304344\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2430: 2.223278\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2435: 1.955679\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2440: 2.137051\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2445: 4.064001\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2450: 3.482255\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2455: 4.084906\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2460: 4.348234\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2465: 8.253658\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2470: 4.565669\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2475: 4.338728\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2480: 3.914818\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2485: 6.880533\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2490: 4.764022\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2495: 2.483950\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2500: 5.836749\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2505: 4.462080\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2510: 3.788981\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2515: 3.004175\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2520: 6.436680\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2525: 6.212205\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2530: 7.038045\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2535: 1.439672\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2540: 8.719888\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2545: 4.606736\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2550: 3.052891\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2555: 1.873073\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2560: 2.255984\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2565: 2.731970\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2570: 2.925481\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2575: 1.916411\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2580: 3.017878\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2585: 5.582393\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2590: 5.595509\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2595: 3.774100\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2600: 3.161583\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2605: 2.268877\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2610: 11.234365\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 2615: 2.551007\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2620: 2.189756\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2625: 10.419140\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2630: 6.221335\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2635: 3.488752\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2640: 5.637297\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2645: 8.283645\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2650: 4.672537\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2655: 6.412217\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2660: 9.068317\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2665: 9.730749\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2670: 3.615239\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2675: 5.965803\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2680: 5.957991\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2685: 3.702473\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2690: 4.822076\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2695: 4.936968\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2700: 2.225877\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2705: 2.615723\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2710: 4.223701\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2715: 2.997949\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2720: 3.303195\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2725: 2.676606\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2730: 2.782970\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2735: 1.946531\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2740: 1.517499\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2745: 4.829917\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2750: 4.280871\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2755: 3.108743\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2760: 3.329384\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2765: 8.688652\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2770: 7.168280\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2775: 0.822449\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2780: 3.899950\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2785: 5.709069\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2790: 1.254507\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2795: 4.776789\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2800: 8.393800\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2805: 6.537826\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2810: 3.824855\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2815: 5.913891\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2820: 4.013646\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2825: 2.120145\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2830: 4.860336\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2835: 3.248388\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2840: 4.381686\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2845: 3.025710\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2850: 1.600921\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2855: 4.157108\n",
      "Minibatch accuracy: 0.0%\n",
      "Minibatch loss at step 2860: 1.208599\n",
      "Minibatch accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "print(valid_prediction.shape)\n",
    "print(valid_labels.shape)\n",
    "with tf.Session(graph=graphRelu) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 5 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
